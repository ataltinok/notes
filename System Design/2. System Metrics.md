# System Metrics

A core principle of system design is **trade-offs**. Every choice comes at a cost we are willing to pay. These costs/metrics help us quantitatively determine whether a design choice was good or bad. We should first understand that each application performs three *high-level* tasks:
1. **Moving data:** Sharing data with other servers throughout the globe or data center.
2. **Storing data:** Keeping data in a persistent storage unit while making sure that unit is reliable, fast and secure.
3. **Transforming data:** Taking raw data and shaping/filtering/parsing it to get intuitive insights.

All our design choices revolve around these three actions. These choices determine how well our application performs under its requirements and how sound we will sleep at night.


## The Cost of Good Design

We use certain metrics to measure how bad (or good) an application is.

1. **Availability:** Defined as the percentage of an application's uptime relative to its lifetime, i.e.
> $availability = \frac{uptime}{uptime + downtime}$

It is measured in '9's. If a server has 99% availability, it is up for 361 days of the year. That is a total of 4 days where the app fails to operate. This might be adequate for a small application but a financial nightmare for Amazon. 

Big companies tend to go for at least 5 '9's, meaning 99.999% availability i.e. 5 mintues of downtime a year. Going for 6 '9's would mean decreasing downtime from 5 minutes to 30 seconds, which is a trivial jump for an incredibly hard availability improvement.

When comparing availabilities, it is better to focus on the downtime. As such, a system with 99.9% availability is only down one tenth of the time of a system if 99% availability. Therefore, adding a '9' is a 10x improvement.

2. **Reliability:** The probability that our server can provide responses under certain conditions for a period of time.

3. **Fault Tolerance:** The ability of our system to heal and keep running under extreme conditions such as DDoS attacks, earthquakes, data center outages, etc.

4. **Redundancy:** A method of fault tolerance that includes having a "redundant" server ready to go when a server has a fault. Usually, we have multiple servers active and these servers are ready to take on each others' load when needed. This is called active-active redundancy. In most cases, active-active is what we refer to when we say "redundancy".

**Note:** During a DDoS attack, when the first server is overloaded and the redundant one takes its place, this new server will also get overloaded. So redundancies are not a perfect solutions.

5. **Throughput:** The amount of data/requests our system can handle per second. Denoted as transactions per second (TPS). Other denotations are queries per second (QPS) for database queries and bytes per second for measuring datapipeline speeds.

6. **Latency:** The delay from the moment the client makes a request to when the client receives a response, AKA round-trip latency. Latency is affected by the network's traffic, the application's code's efficiency and the hardware of the server. 


## Closing Notes

Now that we know what makes a system good, it is easy to say that we want;
1. High availability
2. High throughput
3. Low latency

The goal of this entire course is to learn how to achieve these properties.